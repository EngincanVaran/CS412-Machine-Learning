# -*- coding: utf-8 -*-
"""Engincan Varan HW5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Onv6KIB7s4bRgpoclIuXV77LYKyDrN9

### Make sure GPU is on

https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d
"""

import tensorflow as tf
tf.test.gpu_device_name()

import tensorflow_datasets as tfds
import random 
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from skimage.util import montage

"""### (10 pts.) Prepare the datasets

#### load the plant_village data from tensorflow_datasets. Split by %80-10-10 train-val-test using split= input.

####Â Please check https://www.tensorflow.org/datasets/splits
"""

raw_test = tfds.load('plant_village', split = 'train[:10%]', as_supervised=True)    #to do
raw_val = tfds.load('plant_village', split = 'train[10:20%]',as_supervised=True)    #to do
raw_train = tfds.load('plant_village', split='train[20:100%]',as_supervised=True)   #to do

"""#### Normalize each image into [0,1] range, shuffle and minibatch 128."""

pp_test = raw_test.map(lambda img, label: (tf.image.convert_image_dtype(img/255, tf.float32), label)).shuffle(1024).batch(128)    #to do
pp_val = raw_val.map(lambda img, label: (tf.image.convert_image_dtype(img/255, tf.float32), label)).shuffle(1024).batch(128)                  #to do
pp_train = raw_train.map(lambda img, label: (tf.image.convert_image_dtype(img/255, tf.float32), label)).shuffle(1024).batch(128)                #to do

pp_train.take(1)

"""#### Display the examples from the dataset."""

plt.figure(figsize=(10,10))
for example in pp_train.take(1):  # Only take a single example
  image, label = example[0], example[1]
  plt.imshow(montage(image, multichannel=True))

"""### (20 pts) The Fully Connected Neural Network implementation

#### It is a 4 layer (Fully Connected) Neural Network. The feature depths are [512,256,128] and last softmax layer has 38 output. Train for 10 epochs, with 0.001 learning rate and categorical cross entropy.
"""

model = tf.keras.models.Sequential([
    tf.keras.Input(shape=(256,256,3)),
    tf.keras.layers.Flatten(),                              ### Flatten
    tf.keras.layers.Dense(512, activation="relu", input_shape=(256,256,3)),          ### Dense
    tf.keras.layers.Dense(256, activation="relu", input_shape=(256,256,3)),          ### Dense
    tf.keras.layers.Dense(128, activation="relu", input_shape=(256,256,3)),          ### Dense
    tf.keras.layers.Dense(38, activation='softmax' , input_shape=(256,256,3)),        ### Dense softmax
])

model.compile(
    loss= 'sparse_categorical_crossentropy',                ### loss function sparse_categorical_crossentropy
    optimizer = tf.keras.optimizers.Adam(lr=0.001),                 ### Adam optimizer  Default Learning Rate = 0.001
    metrics = ['accuracy']                                  ### metrics
)

history = model.fit(
    pp_train,
    epochs=10,
    validation_data=pp_val,
)

"""#### Plot training & validation accuracy values"""

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""#### Calculate test set"""

# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(pp_test)
print('test loss, test acc:', results)

pp_val

"""### (20 pts.) The Convolutional Neural Network implementation

#### It is a 4 layer Convolutional Neural Network. First two layers are Convolutional and last two layers are Fully Connected. The depths are [64,128,64] and the last softmax layer has 38 output. Train for 10 epochs, with 0.001 learning rate and categorical cross entropy.
"""

model_cnn = tf.keras.models.Sequential([
    tf.keras.Input(shape=(256,256,3)),
    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='relu', input_shape=(256,256,3)),       ### Conv2D layer
    tf.keras.layers.MaxPooling2D(pool_size=(2,2) , strides=1, padding='same'),                                      ### MaxPooling2D layer
    tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, activation='relu', input_shape=(256,256,3)),      ### Conv2D layer
    tf.keras.layers.MaxPooling2D(pool_size=(2,2) , strides=1, padding='same'),                                      ### MaxPooling2D layer
    
    tf.keras.layers.Flatten(),                                                                                      ### Flatten
    tf.keras.layers.Dense(64, activation='relu', input_shape=(196680,)),                                                                   ### Dense
    tf.keras.layers.Dense(38, activation='softmax', input_shape=(196680,)),                                                                ### Dense softmax
])

model_cnn.compile(
    loss= 'sparse_categorical_crossentropy',                ### loss function sparse_categorical_crossentropy
    optimizer = tf.keras.optimizers.Adam(),                 ### Adam optimizer  Default Learning Rate = 0.001
    metrics = ['accuracy']                                  ### metrics
)

history_cnn = model_cnn.fit(
    pp_train,
    epochs=10,
    validation_data=pp_val,
)

"""#### Plot training & validation accuracy values"""

# Plot training & validation accuracy values
plt.plot(history_cnn.history['accuracy'])
plt.plot(history_cnn.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history_cnn.history['loss'])
plt.plot(history_cnn.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""#### Calculate test set"""

# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model_cnn.evaluate(pp_test)
print('test loss, test acc:', results)