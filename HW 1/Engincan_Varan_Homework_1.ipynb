{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Engincan Varan Homework 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTfDIhAYYjaX",
        "colab_type": "text"
      },
      "source": [
        "# Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGi6c-0uZSeB",
        "colab_type": "code",
        "outputId": "3608d86e-6d49-43bd-8956-4d622500dc3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFpeyxi8aqIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can find the data under https://drive.google.com/drive/folders/1e550az93U3_kfRBbVY5PZnMKYwGYmHqi?usp=sharing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv(\"/content/drive/My Drive/HW1/train_data.csv\")     # One line of code\n",
        "train_label = pd.read_csv(\"/content/drive/My Drive/HW1/train_label.csv\")   # One line of code\n",
        "\n",
        "test_data = pd.read_csv(\"/content/drive/My Drive/HW1/test_data.csv\")       # One line of code\n",
        "test_label = pd.read_csv(\"/content/drive/My Drive/HW1/test_label.csv\")      # One line of code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZXqzZlXBIN0",
        "colab_type": "code",
        "outputId": "4a522050-3428-47ca-c83f-f3a22eb632b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# show random samples from the training data\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(train_data.sample(np.random.randint(1,14)))    # One line of code"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(793, 49)\n",
            "(207, 49)\n",
            "      id  duration  ...  job_unskilled resident  own_telephone_yes\n",
            "519  520         6  ...                       0                  0\n",
            "168  169        24  ...                       0                  1\n",
            "231  232         9  ...                       1                  0\n",
            "44    45        48  ...                       1                  0\n",
            "54    55        36  ...                       0                  1\n",
            "248  249        24  ...                       0                  0\n",
            "462  463        12  ...                       0                  0\n",
            "152  153        36  ...                       0                  0\n",
            "103  104         9  ...                       0                  1\n",
            "\n",
            "[9 rows x 49 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQIVU4-zXpE_",
        "colab_type": "text"
      },
      "source": [
        "# Train Decision Tree with default parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLznLl_lXYZf",
        "colab_type": "code",
        "outputId": "bcc03d9e-06d3-4cdf-e5a0-14085be57e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Train decision tree using the whole training data with **entropy** criteria\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy')              # One line of code\n",
        "dt = dt.fit(train_data, train_label)                          # One line of code\n",
        "\n",
        "\n",
        "# Estimate the prediction of test data\n",
        "test_pred = dt.predict(test_data)                              # One line of code\n",
        "\n",
        "# Calculate accuracy of test data\n",
        "from sklearn.metrics import accuracy_score\n",
        "TestAcc = accuracy_score(test_label, test_pred)                 # One line of code\n",
        "print(\"Testing Accuracy = %.5f%%\" % (TestAcc * 100))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy = 68.59903%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqgNZYUMXv8X",
        "colab_type": "text"
      },
      "source": [
        "# FineTune Decision Tree parameters\n",
        "\n",
        "1- Spliting dataset into train and validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWJxk-zjy0Kv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split training data to 70% training and 30% validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_data, train_label, train_size = 0.7)   # One line of code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqws-kTZYHoG",
        "colab_type": "text"
      },
      "source": [
        "2- FineTune minimum sample split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1DvpmCCJXTb",
        "colab_type": "code",
        "outputId": "b090f232-7abd-4f2b-bfe0-c75f855fc355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "min_samples_splits = range(2, 100)\n",
        "\n",
        "train_results = []\n",
        "val_results = []\n",
        "for min_samples_split in min_samples_splits:\n",
        "  \n",
        "  # Fit the tree using the 70% portion of the training data\n",
        "\n",
        "  dt = DecisionTreeClassifier(criterion='entropy', min_samples_split= min_samples_split)\n",
        "  dt = dt.fit(x_train, y_train)\n",
        "\n",
        "  # Evaluate on Training set\n",
        "  train_pred = dt.predict(x_train)                    # One line of code\n",
        "  train_acc = accuracy_score(y_train, train_pred )    # One line of code\n",
        "  train_results.append(train_acc)\n",
        "   \n",
        "  # Evaluate on Validation set\n",
        "  val_pred = dt.predict(x_val)                        # One line of code\n",
        "  val_acc = accuracy_score(y_val, val_pred)\n",
        "  val_results.append(val_acc)\n",
        "  \n",
        "# Ploting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(min_samples_splits, train_results, 'b')\n",
        "plt.plot(min_samples_splits, val_results,'r')\n",
        "plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5xU9b3/8deHpYh1QVZQQLBgQWPB\nVRBMUGyoCBobiB0lRkVs8VIUkQtijFcJityoKDZEUYNIbPcHGowRZYmKhSJgULBtABW9xqV87x+f\n2d8uW2d3Z/bMnnk/H495sKfMOZ/ZAx++860WQkBERBq+RlEHICIiqaGELiISE0roIiIxoYQuIhIT\nSugiIjHROKobt2rVKnTs2DGq24uINEgLFy78Vwghr6JjkSX0jh07UlBQENXtRUQaJDNbVdkxVbmI\niMSEErqISEwooYuIxIQSuohITCihi4jERLUJ3cweMrNvzOzDSo6bmU00s+VmtsjMuqQ+TBERqU4y\nJfSpQO8qjp8EdEq8BgOT6x6WiIjUVLUJPYQwD1hXxSn9gEeDmw/kmtmuqQqwrE8+geHDQbP+iohs\nLRV16G2Bz0ttr07sK8fMBptZgZkVFBYW1upmzz8Pt98Ow4bV6u0iIrFVryNFQwj3A/cD5Ofn16qM\nff318OmncMcd0Lo1XHddSkMUEWmwUpHQ1wDtS223S+xLCzOYOBG++caTe+vWMHBguu4mItJwpKLK\nZRZwQaK3SzfguxDClym4bqVycuDxx+GYY+D886FbNxg1CubPT+ddRUQyWzLdFp8E3gL2NbPVZjbI\nzC43s8sTp7wIrASWAw8AV6Qt2lKaNfP69NGjoVEjGDcOjjzS94mIZCOLapHo/Pz8kMrZFtevh4MO\ngoMPhtmzU3ZZEZGMYmYLQwj5FR2LzUjRFi28Lv3ll71+XUQk28QmoYPXp2/eDNOnRx2JiEj9i1VC\nP+AAOPRQeOyxqCMREal/sUroAOedBwUFsGRJ1JGIiNSv2CX0AQO814tK6SKSbWKX0HfdFY4/Hp54\nArZsiToaEZH6E7uEDt44umoVvPFG1JGIiNSfWCb0006Dli199KhmZRSRbBHLhL7ddj4j47x5MG1a\n1NGIiNSPWCZ0gEGD4IgjfAKv776LOhoRkfSLbUJv1Ajuu89HjY4aFXU0IiLpF9uEDnDYYfDb38K9\n98L770cdjYhIesU6oQOMHesNpMOHRx2JiEh6xT6ht2gBQ4fCSy/BRx9FHY2ISPrEPqGDV7s0bw53\n3RV1JCIi6ZMVCX3nneGSS3yVoy/TupaSiEh0siKhA1x7LWzc6A2kIiJxlDUJfa+94Ne/hsmT4Ycf\noo5GRCT1siahA9xwgy9Vd999UUciIpJ6WZXQu3WD446D//gP6NdPc6aLSLxkVUIHmDULxo+H11+H\nAw/0BTGmT4e1a6OOTESkbrIuoTdvDsOGwfLlcMUV8OKLvihGXh6cdBL8/HPUEYqI1E7WJfRieXkw\ncSIUFsL8+XDjjfDyy3DnnVFHJiJSO42jDiBqOTnQtau/VqyAceNg4EDo2DHqyEREaiZrS+gVuftu\nn6Vx6NCoIxERqTkl9FLatfOpdmfNgtmzo45GRKRmLES0Rlt+fn4oKCiI5N5VKSqCQw7xRTF69fJ9\nzZrBZZd5tYyISJTMbGEIIb+iYyqhl9G0KTz0kM//8ve/++uZZ7wP+znnwMqVUUcoIlIxJfQKdOsG\nixZ5I+mKFfD5514VM3s27LcfzJgRdYQiIuUpoSdhhx3g1lu97/phh/l6pSqpi0imUUKvgV139VGl\njRrBuef67I3Fli3T1LwiEi0l9Brq0AEeeADefhtuuQVWr4aLLvKqmB49vDFVRCQKSui1cNZZcOml\ncPvt0KmTl9ovvhhWrfLVkSLqOCQiWS7rR4rW1oQJ3mDatq0vRN2hg48uHTUKTjwRLrww6ghFJNuo\nH3oKbd7sfdcXLoR//AP22SfqiEQkbtQPvZ7k5MATT/hApAsvVNWLiNQvJfQUa9cO/vAHn8Fx1qyo\noxGRbKKEngYXXODVLTffDFu2RB2NiGQLJfQ0aNzYByJ98AE89VTJ/lWr4L//2+vaRURSLamEbma9\nzWypmS03s2EVHO9gZnPMbJGZvW5m7VIfasNy9tnwi194X/VNm2DePMjP926N48dHHZ2IxFG1Cd3M\ncoBJwElAZ2CAmXUuc9qdwKMhhIOAMUDWp6xGjeA//xM++cRHlR53nE/4deqpMHq0T/olIpJKyZTQ\njwCWhxBWhhCKgOlAvzLndAbmJn5+rYLjWalvXzj8cJ/Mq1cvbyh9/HHvs37uufDtt1FHKCJxkkxC\nbwt8Xmp7dWJfae8Dv078fDqwg5ntXPZCZjbYzArMrKCwsLA28TYoZvDoo3DvvT5TY24u7LgjTJsG\na9bA4MHq2igiqZOqRtEbgJ5m9i7QE1gDlGv6CyHcH0LIDyHk5+XlpejWmW2//eDKK72htFjXrj66\ndMYMuPxyX1RDRKSukhn6vwZoX2q7XWLf/xdC+IJECd3MtgfOCCGoQqEKv/udT+Q1fjwsXeqLaLRq\nFXVUItKQJVNCXwB0MrM9zKwp0B/YasiMmbUys+JrDQceSm2Y8dOoEdx2m9epz58PRxzhU/CKiNRW\ntQk9hLAJuAp4BVgMPB1C+MjMxphZ38RpRwNLzWwZ0BoYl6Z4Y2fgQO/S+MMPcMIJ8MUXUUckIg2V\nJufKEAsXwtFHwx57eILPzY06IhHJRJqcqwE47DB47jlYssS7O/70U9QRiUhDo4SeQY4/3rs5vvEG\njBgRdTQi0tAooWeY/v19Eer77oPPP6/+fBGRYkroGWjUKP9z7Nho4xCRhkUJPQPtvruPIn3oIV/m\nTkQkGUroGWrkSGjSxCfyEhFJhhaJzlBt2sCQIb760THHwKJF8D//49MEHH+891nPz996SoGKNG0K\nLVvWT8wiEi31Q89ga9d6v/QNG2CbbaBnT0/Qr73mA5GSddVVcPfd1Sd/Ecl8VfVD1z/xDLbzzjB3\nLqxfD0cdBc2b+/6iInjrLVi8uPprvPeez/a4ZAk8/TS0aJHemEUkOkroGS6/gv+Hmzb10nrPnsld\no2tX+M1v/M/bb4djj4WddkptnCISPTWKZoGLL/Zqmg0b4IwzvOR/1FHw7LNRRyYiqaSEniV69IDP\nPoO//hWGDYN16+DMM+Hmm2HLFj/n+++9D/ygQT6lr4g0LGoUzVJFRXDFFTBlCpx+ui+RN2YMFBZ6\nXX1RkVfTjBhRcb17Tg40a1b/cYtkO03OJeU0bQoPPAATJsDzz3sXyQMOgAUL4J//9GT+pz9Bu3aw\n3XYVv8aM0RJ6IplEJXThzTfhxx+9f7tZyf6lS+GFF0qqZEpbsMBXWTr7bHj4Ydh22/qLVySbqdui\nVKlHj4r377uvvyoSgg96GjbMpyeYPdsHQ4lIdFTlIrViBjfeCLNmwccf+9wzIhItJXSpkz594NZb\nvWrmhReijkYkuymhS51dcw107gxXX62VlkSipIQuddakCUya5L1jxo+POhqR7KVGUUmJo4+GgQPh\n97+HLl0qnlpg//3VcCqSTkrokjJ33gl/+YsPVKpIixY+Wdjuu9dvXCLZQgldUqZNG58BsqJpAzZs\ngAED4LzzfF6ZnJz6j08k7pTQJaXatKm8WmXyZDj/fBg3rmTdVBFJHTWKSr057zxP6LfeCn/7W9TR\niMSPErrUq0mTfBWmgQN94Q4RSR0ldKlXO+wATz4JX3wBl12myb1EUkkJXerd4YfDbbf5AhsPPhh1\nNCLxoYQukbj+ep/dcehQnwtGROpOvVwkEo0awaOPwkEHQb9+PjCprJwcOOccOOaYkn0//eS9ZZJZ\nIDsvz+dsb6y/5ZIl9FddItOmDUybBpdfDi++WP74Dz/4IhunnOKLW//jHzByJKxe7e9tVMX3y82b\n4euvYe+94ZJL0vcZRDKJFriQjPXvf8PEiV7f/t13vi8/30ek9uxZ9XtDgK5dPakvW6bl8iQ+tASd\nNEjbbONzri9fDmPHwvTp8Pbb1Sdz8Pnax471hbHV8CrZQiV0ia0QvG5+2TJfVamiZfIWL4bddqt4\nMjGRTKQSumSl4lL6V1/Bffdtfax4Cb0DDoC99vKqnaKiaOIUSRU1ikqs/fKXcOKJ3qi6775eYm/S\nxJfMe+wxnxny+++9++Q990DfviULZXftCmedFWn4IjWiKheJvffe866P337rybx1a+8pM2YM3HST\nn/Pyy96DZtky39682RtlhwyBu+5S10fJHFVVueivqcTeIYd4tcubb8Krr8LChTBhApxxRsk5J53k\nr2KbNnmD7N13w5Il8NRTPp+7SCZTCV2kCg895P3kO3b0RbD33TfqiCTb1blR1Mx6m9lSM1tuZsMq\nOL67mb1mZu+a2SIzO7muQYtkgksugblzvbqma1cv4YtkqmoTupnlAJOAk4DOwAAz61zmtJuAp0MI\nhwL9gTJ9CkQarqOOggULoEMHr5aZODHqiEQqlkwJ/QhgeQhhZQihCJgO9CtzTgB2TPy8E/BF6kIU\niV6HDl4Hf+qp3iPmrbeijkikvGQSelvg81LbqxP7ShsNnGdmq4EXgSEVXcjMBptZgZkVFBYW1iJc\nkehsvz088QTssktJ7xiRTJKqgUUDgKkhhHbAycBjZlbu2iGE+0MI+SGE/Ly8vBTdWqT+bLcdDB/u\n9epz50YdjcjWkknoa4D2pbbbJfaVNgh4GiCE8BawDdAqFQGKZJrLL4d27byUXtxJbN48H3W6224V\nv4YM0UhUSb9k+qEvADqZ2R54Iu8PnFvmnM+AY4GpZrY/ntBVpyKxtM02cPPN8Jvf+LS/X3wBV1zh\na6X26VP+/PXr4d57YdEiX6WplYo6kiZJ9UNPdEOcAOQAD4UQxpnZGKAghDAr0evlAWB7vIH0xhBC\nlR281A9dGrKNG2G//WDtWp/at3dvXys1N7fi86dNg0GDfB7322+H5s2Tv9c22/hI1yZNUhO7NGxV\n9UPXwCKRWnrySTj3XLjuOrjjDl9hqSoLFvjqTF9+WfN79eoFM2ZAy5a1i1XiQwldJE2++cZ7vSRr\nwwb45JOa3eOdd7yr5O67w6xZsP/+NXu/xIvmchFJk5okc4AddoAuXWr2ni5dfO3V00+Hbt2gR4+a\nvV8yz1VXwclpGE+vhC7SAHTv7lU2Q4bUrspGMstPP6XnukroIg3E7rvD889HHYVkMq1YJCISE0ro\nIiIxoYQuIhITSugiIjGhhC4iEhNK6CIiMaGELiISE0roIiIxoYQuIhITSugiIjGhhC4iEhNK6CIi\nMaGELiISE0roIiIxoYQuIhITSugiIjGhhC4iEhNK6CIiMaGELiISE0roIiIxoYQuIhITSugiIjGh\nhC4iEhNK6CIiMaGELiISE0roIiIxoYQuIhITSugiIjGhhC4iEhNK6CIiMaGELiISE0roIiIxoYQu\nIhITSugiIjGhhC4iEhNJJXQz621mS81suZkNq+D43Wb2XuK1zMy+TX2oIiJSlcbVnWBmOcAk4Hhg\nNbDAzGaFED4uPieEcG2p84cAh6YhVhERqUIyJfQjgOUhhJUhhCJgOtCvivMHAE+mIjgREUleMgm9\nLfB5qe3ViX3lmFkHYA9gbiXHB5tZgZkVFBYW1jRWERGpQqobRfsDz4QQNld0MIRwfwghP4SQn5eX\nl+Jbi4hkt2QS+hqgfantdol9FemPqltERCKRTEJfAHQysz3MrCmetGeVPcnM9gNaAG+lNkQREUlG\ntQk9hLAJuAp4BVgMPB1C+MjMxphZ31Kn9gemhxBCekIVEZGqVNttESCE8CLwYpl9o8psj05dWCIi\nUlMaKSoiEhNK6CIiMaGELg3D11/Dzz/X/v0a9yBZQAldMt+//gX77gu//W3t3v/cc7DLLjB5cmrj\nEskwSTWKZrSvv4bWraOOQoqtWwerVlV8rF07qM2Ast//Hr77Dh55BH73O9h//+Tfu3kzjBzpPw8d\nCl26QNeuNY9BpAFo2CX0WbNgt91g8eKoIxGAFStg7709aVb06tgR7rrLk2yyvvwS7r0X+vSBbbeF\nW26pWUyPPw5LlsADD0DbtnDmmap+kdhq2CX06dNhyxZ4/fWaldok9f73f+GMM/znp56CZs22Ph4C\nTJkC11/vz+3BB+Ggg6q/7rhxsGkTTJgAU6fC2LHw7rtwaBITehYVwejR/p/JoEH+Z/fuMGAAvPIK\n5OTU9FOKZLYQQiSvww47LNTJzz+HsOOOIUAI559ft2tJ3WzZEsKFF4ZgFsKLL1Z93vTpIeTlhdC4\ncQgjR4bw00+Vn//ppyE0aRLC4MG+vX59CLm5IfTpk1xckyf734/SMU2Z4vsmTEjuGiIZBigIleRV\nCxEN7MzPzw8FBQW1v8Crr8KJJ0LLlv765JPUBSflhQCffuql3rJeegmuu86rQ0aPrv5aa9f6+Y8+\n6o2df/wjdOhQ/ryxY+GZZ2D5cq9/Bxg/HkaMgJkz/b2V2bwZTjgB9tgD3ngDzEqOHXssfPABrFwJ\n229ffbwbN/q5xf9WcnOhTZvq3yeSBma2MISQX+HByjJ9ul91LqFfcUUI224bwi23eImrsLBu15PK\nrVoVwskn+++5slfv3iFs3lyz677ySggdO1Z93Wuu2fo9P/wQwi67VP2e0q/XXy9/37//3Y/ddlty\ncV566dbXbNQohBtuCOHHH2v2eUVSgNiV0EOA9u29t8LVV8PRR8MLL3jDmdTcTz/Bmkom0Hz5ZRg+\n3NsqRoyAvfYqf06TJnDSSd5oWVM//uj32Lix/LHGjeHkk8tfd/FieP/96q+dl+el8Yr06QNvvunf\nOnJzK7/G0qXQuTOcfTb0S6zrMmeOtwHsuSfcf3/l96hI2d9169awww5bnxMCrF5d0u9+22298V+E\nOJbQFyzwktIjj3iJLScnhBEjan+9bPbccyHsumvVpdwTTvD67Dh5913/bDfdVPV555wTwnbbhfD1\n11vvnzs3hL339mvMm5fcPbdsCaFHj61/t3vuWf6bzaxZ5Z/Bvfcm/9kk1qiihN4we7nMnOk9FE45\nBbbbDg4+GN7SrL3V+vZbf4H3Shk1Cp591n9/48Z5Sbus1q3huOO2roOOg0MOgbPO8t4zV19dcf/4\nRYu8x86IET4wqbRjjoH33oN99vHj8+ZV/zuaPdu/FVx3nffSWbjQ779wIRx+eMl5Tz0FrVrB3Xf7\n9uOPwzXX+Hu6d6/b55Z4qyzTp/tVpxL6AQeEcMwxJdtXXumlqI0ba3/NOCsq8vriZs22LvU1a+b7\ni4qijjAaH3/s9eH5+SEsWlT+eN++Iey0Uwjr1lV+jfvu89/lSy9Vfa/Nm0M46CAv1Rf/vteuLf/t\nsqjI73nxxSX71q/3kvxuu4Xw1VfJfz6JJWJVQl++HD76CC67rGTfkUfCpEnw4Yde8pISBQVw6aVe\n53zGGVu3M/zylxXXiWeL/feHp5/2KQW6dIFhw2DwYGjUCD7+2AeujR0LLVpUfo1Bg+COO+Cmm7zX\nVWWl9BkzvMT/+OMl34RatoRf/cq/cY4b5/v++lcfFXvaaSXvzc31b1JHHgn9+8Njj8XvG1O2yc31\n2oVUqyzTp/tV6xL6nXd6iah0ne6KFb7vvvtqd824mjPH+3vvtlsIf/5z1NFkrsLCEC64oHy9datW\nIXz/ffXvf/hhP//ZZys+vnFjCPvs498sN23a+tiECf7eZct8+6qrQmjevOIeNFOnVt3WoVfDeU2e\nXKO/oqURq14uS5d6L4MrrijZF4L3Cz7xRO/bLN5LoksXr4t9882qS5ni/va3raeRyM9PbkTqpk1w\n4IHerjNnTvnS83PP+d/X556D00/f+tg//+l95f/wBx9Fu/vuft8//7nie736auVz5UjD0aOH956q\nhap6uTS8hF6Z007zr8nLlqXumg1VURH07OlVUAsWwH77RR1R/D39NJxzTuXH8/PhnXcqrio59FD/\n+v3HP/p5U6fChRemLVRp2KpK6A2vDr0yRx4Jzz/vU622ahV1NPXv229LJr0aNQrmz/d6WyXz+nHW\nWd5Xf/36io9XVb/erx+MGeMTiDVq5L23RGohPgm9Rw//c+hQn50vW6oY1qyBK6/0/8xKu+46n1lQ\n6oeZN1jWxmmnwa23+iClX/0qOwskkhLxSugjR8Ltt3s95j33eKkI/B/b9ts3zJ4Bmzf7aMqyQvBZ\nC2+80UdZDh8Ou+7qx1q2rPrrv2SWgw/2uWxWrSoZjSpSC/FJ6GbexezMM70r2dlnb3185Eg/3pCE\nAEcd5dUnlenVy0t22dz9sKEz80Q+caISutRJfBpFS9u0yUfbff21bz/5JHzzjfcoaEil9Hfe8flq\nLroIfvGL8sc7dvReEw3pM0nF1q6Ft9/2uWtEqpAdjaKlNW4MAweWbOfmeqn9/fcb1sCjKVOgeXMf\nHr7TTlFHI+m0885K5lJnDXsJumSdeqr3Hpg5M+pIkvfjj/7N4uyzlcxFJCnZkdDz8rzRtCEl9Bkz\nYMMGH7YvIpKE7Ejo4I1N77/v9egNwYMP+oo8xd0xRUSqkV0JHcr3166JZct8VfsZM1ITU2WWLPHh\n+oMGqcFTRJKWPQl9773hgANqX+3y44/w61/DihVwww0lq8mkw5Qp3rB7wQXpu4eIxE72JHTwEXnz\n5nkXsZUrvVdBRcvWvfGGT5J0770+nDsEn6538WIfwPPZZz5Muy4++MAHk4wdu/XCyzNn+rVPPdUX\nlxARSVZl0zCm+1XnRaJro3jpur59fYrS4qksly/f+ryLLio51r17CMOG+c/jxvkyYj17htCmTd0W\nCR48OAQzv+6BB4bwl7+EcMYZvn3wwSXTqYqIlEIV0+dmVwn9sMOgbVtfuOC447y0DlvXq2/a5AtO\nn3uuT8W7ZIlPJ3Dqqb4AgpkvRvDVV16Cr43iLokXXOD3Xr/eJ2SaPRvGj/cZEjt1qvvnFZGsEs+R\nolWZNw++/94TqJnPo5Gb6yvFFB/v2dMbPs8800eYTpsGF1+8dX/wk0/2kX2ffgo77lizGKZO9eu9\n8YYP7f/+e+/V0qePr1EpIlKJqkaKZlcJHXw2uz59SnqPnHaaL2xQWOjbM2dCs2YlE3vtsosv0Ft2\ncM/YsbBuXclCvjVRtkvijjv67IhK5iJSB9mX0Mvq188bPmfP9lrzmTPh2GNhhx2qfl+XLr5G53/9\nlzeyJktdEkUkTZTQDz0U2rf3uuwPP/QqlNIL9FZlzBj44QdfJLgy06bB4Yf7lL6gLokikjZK6Gae\nwF991ZOvmTeAJqNzZ58E7J574MsvKz7noYegoMAbYS+5xBta1SVRRNJACR282uWnn7w+vFs3X3A6\nWaNHez/y224rf+zf//bqlcsv9x4yjz7qjayDBqUsdBGRYkro4A2lubk++jPZ6pZie+3lJe8//an8\nauzz53tSP+UU745YUAB33QW9e6cudhGRhKQSupn1NrOlZrbczIZVcs7ZZvaxmX1kZtNSG2aaNWlS\nMmK0pgkd4OabvaqmbCl9zhzIyfH/MMDnYr/2Wt8nIpJi1SZ0M8sBJgEnAZ2BAWbWucw5nYDhQI8Q\nwgHANWmINb1uusmXAKtN18H27eH88+GJJ3zK22Jz50J+fs37qYuI1EIyJfQjgOUhhJUhhCJgOlB2\n4cPLgEkhhPUAIYRvUhtmPdh3XxgypPbvv/RSHwH61FO+vWGDLyF37LGpiU9EpBrJJPS2wOeltlcn\n9pW2D7CPmb1pZvPNrMJKYjMbbGYFZlZQWDyQJy66dvXZHB980LffeMOnEejVK9q4RCRrpKpRtDHQ\nCTgaGAA8YGa5ZU8KIdwfQsgPIeTn5eWl6NYZwsx7r7z9tvdnnzvXR5x27x51ZCKSJZJJ6GuA9qW2\n2yX2lbYamBVC2BhC+BRYhif47HL++d7AOmWKN4h27+6LPIuI1INkEvoCoJOZ7WFmTYH+wKwy58zE\nS+eYWSu8CmZlCuNsGFq1gtNPh4cfhvfeU3WLiNSrahN6CGETcBXwCrAYeDqE8JGZjTGzvonTXgHW\nmtnHwGvA70IINZjgJEYGDYLvvvOf1SAqIvUo+6bPTbctW2DPPX3CrnXrvApGRCRFqpo+t3F9BxN7\njRrBpEk+t4uSuYjUIyX0dDjllKgjEJEspLlcRERiQgldRCQmlNBFRGJCCV1EJCaU0EVEYkIJXUQk\nJpTQRURiQgldRCQmIhv6b2aFwKpqTmsF/KsewslE+uzZJ1s/N+iz1+SzdwghVDj/eGQJPRlmVlDZ\nnAVxp8+efZ89Wz836LOn6rOrykVEJCaU0EVEYiLTE/r9UQcQIX327JOtnxv02VMio+vQRUQkeZle\nQhcRkSQpoYuIxETGJnQz621mS81suZkNizqedDGz9mb2mpl9bGYfmdnQxP6WZvY/ZvZJ4s8WUcea\nLmaWY2bvmtnsxPYeZvZ24tk/lVicPHbMLNfMnjGzJWa22MyOzIbnbmbXJv6uf2hmT5rZNnF95mb2\nkJl9Y2YfltpX4TM2NzHxO1hkZl1qer+MTOhmlgNMAk4COgMDzKxztFGlzSbg+hBCZ6AbcGXisw4D\n5oQQOgFzEttxNRRfgLzY74G7Qwh7A+uBQZFElX5/BF4OIewHHIz/DmL93M2sLXA1kB9COBDIAfoT\n32c+FehdZl9lz/gkoFPiNRiYXNObZWRCB44AlocQVoYQioDpQL+IY0qLEMKXIYR/JH7egP+jbot/\n3kcSpz0CnBZNhOllZu2AU4AHE9sG9AKeSZwSy89uZjsBvwKmAIQQikII35Idz70x0NzMGgPbAl8S\n02ceQpgHrCuzu7Jn3A94NLj5QK6Z7VqT+2VqQm8LfF5qe3ViX6yZWUfgUOBtoHUI4cvEoa+A1hGF\nlW4TgBuBLYntnYFvQwibEttxffZ7AIXAw4nqpgfNbDti/txDCGuAO4HP8ET+HbCQ7HjmxSp7xnXO\ne5ma0LOOmW0PPAtcE0L4vvSx4H1LY9e/1Mz6AN+EEBZGHUsEGgNdgMkhhEOBHylTvRLH556oL+6H\n/4e2G7Ad5askskaqn3GmJvQ1QPtS2+0S+2LJzJrgyfyJEMJzid1fF3/dSvz5TVTxpVEPoK+Z/ROv\nVuuF1yvnJr6OQ3yf/WpgdeutGYsAAAE7SURBVAjh7cT2M3iCj/tzPw74NIRQGELYCDyH/z3Ihmde\nrLJnXOe8l6kJfQHQKdHy3RRvNJkVcUxpkagzngIsDiHcVerQLODCxM8XAs/Xd2zpFkIYHkJoF0Lo\niD/juSGEgcBrwJmJ0+L62b8CPjezfRO7jgU+Jv7P/TOgm5ltm/i7X/y5Y//MS6nsGc8CLkj0dukG\nfFeqaiY5IYSMfAEnA8uAFcDIqONJ4+c8Cv/KtQh4L/E6Ga9LngN8Avw/oGXUsab593A0MDvx857A\nO8ByYAbQLOr40vSZDwEKEs9+JtAiG547cCuwBPgQeAxoFtdnDjyJtxVsxL+VDarsGQOG9+5bAXyA\n9wSq0f009F9EJCYytcpFRERqSAldRCQmlNBFRGJCCV1EJCaU0EVEYkIJXUQkJpTQRURi4v8AkTK8\nycfCNVgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akVAE3MbL7bE",
        "colab_type": "code",
        "outputId": "af151569-38ea-44bf-af12-f4bc95bab12c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Choose the best minimum split sample based on the plot\n",
        "val_results2 = val_results[20:] #to prevent overfitting\n",
        "Best_minSampl = min_samples_splits[val_results2.index(max(val_results2))]  # One line of code\n",
        "#print(Best_minSampl)\n",
        "\n",
        "# Train decision tree using the full training data and the best minimum split sample\n",
        "dt = DecisionTreeClassifier(criterion='entropy', min_samples_split= Best_minSampl)  # One line of code\n",
        "dt = dt.fit(x_train, y_train)                                                       # One line of code\n",
        "  \n",
        "\n",
        "# Estimate the prediction of the test data\n",
        "test_pred = dt.predict(test_data)                     # One line of code\n",
        "\n",
        "# Calculate accuracy of test data\n",
        "TestAccFineTuned = accuracy_score(test_label,test_pred)            # One line of code\n",
        "print(\"Testing Accuracy = %.5f%%\" % (TestAccFineTuned * 100))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy = 72.46377%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VciE0lfYKhya",
        "colab_type": "text"
      },
      "source": [
        "# Now, apply the same procedure but using KNN instead of decision tree \n",
        "\n",
        "# For finetuning, find the best value of K to use with this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdBaXNKoLBcL",
        "colab_type": "code",
        "outputId": "6a3d0e1b-d9a9-4511-adce-3fc3df267bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# initialize the values of k to be a list of odd numbers between 1 and 30\n",
        "kVals = range(1, 30, 2)\n",
        "\n",
        "# Save the accuracies of each value of kVal in [accuracies] variable\n",
        "# hint: you can use accuracies.append(...) function inside the loop\n",
        "accuracies = []\n",
        "\n",
        "#(x_train, x_val, y_train, y_val) = train_test_split(train_data, train_label, test_size=0.3)\n",
        "\n",
        "# loop over values of k for the k-Nearest Neighbor classifier\n",
        "for k in kVals:\n",
        "  # Follow what we did in decision tree part\n",
        "\n",
        "  knn = KNeighborsClassifier(n_neighbors=k)\n",
        "  knn.fit(x_train, y_train.values.ravel())\n",
        "\n",
        "  score = knn.score(x_val,y_val)\n",
        "  print(\"For k = %d, validation accuracy = %.5f%%\" % (k, score * 100))\n",
        "  accuracies.append(score)\n",
        "\n",
        "# Train KNN using the full training data with the best K that you found\n",
        "BestK = kVals[accuracies.index(max(accuracies))]\n",
        "#print(BestK)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=BestK)\n",
        "knn.fit(x_train, y_train.values.ravel())\n",
        "ValScore = knn.score(x_val,y_val)\n",
        "print(\"Validation Accuracy = %.5f%%, for the best k=%d\" % (ValScore * 100,BestK))\n",
        "\n",
        "# Testing\n",
        "TestScore = knn.score(test_data,test_label)\n",
        "print(\"Test Accuracy = %.5f%%, for k=%d\" % (TestScore * 100,BestK))\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For k = 1, validation accuracy = 59.66387%\n",
            "For k = 3, validation accuracy = 60.92437%\n",
            "For k = 5, validation accuracy = 63.86555%\n",
            "For k = 7, validation accuracy = 63.44538%\n",
            "For k = 9, validation accuracy = 63.44538%\n",
            "For k = 11, validation accuracy = 64.28571%\n",
            "For k = 13, validation accuracy = 65.12605%\n",
            "For k = 15, validation accuracy = 66.80672%\n",
            "For k = 17, validation accuracy = 69.32773%\n",
            "For k = 19, validation accuracy = 68.90756%\n",
            "For k = 21, validation accuracy = 69.32773%\n",
            "For k = 23, validation accuracy = 68.90756%\n",
            "For k = 25, validation accuracy = 68.48739%\n",
            "For k = 27, validation accuracy = 68.48739%\n",
            "For k = 29, validation accuracy = 68.90756%\n",
            "\n",
            "\n",
            "Validation Accuracy = 69.32773%, for the best k=17\n",
            "Test Accuracy = 71.01449%, for k=17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMEdOIlJQBin",
        "colab_type": "text"
      },
      "source": [
        "# Bonus\n",
        "\n",
        "# Apply gridsearch using decision tree on any hyperparameter(s) of your choice, you have to beat your previous obtained accuracies to get the bonus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-9TxVbXQCw7",
        "colab_type": "code",
        "outputId": "d4a4c11c-010b-4d3f-9bbd-79dd0e5576e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Write your code here\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "#                      max_depth=None, max_features=None, max_leaf_nodes=None,\n",
        "#                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#                      min_samples_leaf=1, min_samples_split=2,\n",
        "#                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
        "#                      random_state=None, splitter='best')\n",
        "#'min_weight_fraction_leaf': [0.0,0.2,0.25,0.3,0.5]}\n",
        "#'criterion' : ['gini', 'entropy']\n",
        "#'min_samples_leaf' : range(2,30)\n",
        "#'min_weight_fraction_leaf' : [0.0,0.2,0.5]\n",
        "\n",
        "#tempHyper = [[ [0.0 for k in range(20) ] for i in range(100)] for j in range(20)]\n",
        "tempHyper = [ [0.0 for i in range(100)] for j in range(20)]\n",
        "\n",
        "maxDepth = range(1,20)\n",
        "minSplit = range(2,100)\n",
        "\n",
        "for i in maxDepth:\n",
        "  for k in minSplit:\n",
        "    dt = DecisionTreeClassifier('entropy', max_depth=i, min_samples_split=k)\n",
        "    dt.fit(x_train,y_train)\n",
        "    \n",
        "    tempPred = dt.predict(x_val)\n",
        "    score = accuracy_score(y_val,tempPred)\n",
        "    tempHyper[i][k] = score\n",
        "\n",
        "#print(tempHyper)\n",
        "\n",
        "maxScore = np.amax(tempHyper)\n",
        "sols = np.argwhere(tempHyper==maxScore)\n",
        "#print(sols)\n",
        "depth = sols[0][0]\n",
        "split = sols[0][1]\n",
        "print(\"*** BRUTE-FORCE ***\")\n",
        "print(\"Validation Accuracy = %.5f%%\" % (maxScore * 100))\n",
        "\n",
        "#testing the data with my approach\n",
        "dt = DecisionTreeClassifier('entropy', max_depth=depth, min_samples_split=split)\n",
        "dt = dt.fit(train_data,train_label)\n",
        "testPred = dt.predict(test_data)\n",
        "maxScore = accuracy_score(test_label,test_pred)\n",
        "print(\"Testing Accuracy = %.5f%%\" % (maxScore * 100))\n",
        "\n",
        "#GridSearch Algorithm\n",
        "grid_param= { 'max_depth': range(1,21), 'min_samples_split': range(2,100) }  \n",
        "dt = DecisionTreeClassifier('entropy')\n",
        "dt = GridSearchCV(dt, grid_param)\n",
        "dt = dt.fit(train_data,train_label)\n",
        "#dt = dt.fit(x_train,y_train)\n",
        "\n",
        "GridPred = dt.predict(test_data)\n",
        "\n",
        "GridAcc = dt.best_score_\n",
        "print(\"\\n*** Grid Search ***\")\n",
        "print(\"Testing Accuracy = %.5f%%\" % (GridAcc * 100))\n",
        "\n",
        "ResultAccuracies = [GridAcc, TestAcc, TestAccFineTuned, TestScore, maxScore]\n",
        "if(max(ResultAccuracies) == GridAcc or max(ResultAccuracies) == maxScore):\n",
        "  if(maxScore > GridAcc):\n",
        "    print(\"\\nBONUS is Acquired by using Brute-Force!\")\n",
        "  else:\n",
        "    print(\"\\nBONUS is Acquired by using GridSearch!\")\n",
        "else:\n",
        "  print(\"\\n!!! FAIL - NO BONUS !!!\")\n",
        "\n",
        "#results for all accuracies\n",
        "print(\"\\n*** ALL RESULTS ***\")\n",
        "print(\"Default Decision Tree Accuracy = \\t%.5f%%\" % (TestAcc * 100))\n",
        "print(\"Fine-Tuned Decision Tree Accuracy = \\t%.5f%%\" % (TestAccFineTuned * 100))\n",
        "print(\"KNN Accuracy = \\t\\t\\t\\t%.5f%%\" % (TestScore * 100))\n",
        "print(\"GridSearch Accuracy = \\t\\t\\t%.5f%%\" % (GridAcc * 100))\n",
        "print(\"Brute-Force Accuracy = \\t\\t\\t%.5f%%\" % (maxScore * 100))\n",
        "\n",
        "#what changes we did with the GridSearch vs Mine\n",
        "print(\"\\n*** CHANGES with GridSearch ***\\t\\t\\t\\tGrid vs. Brute-Force\")\n",
        "print(\"Changed hyperparameter is --> max_depth = \\t\\t\" + str(dt.best_estimator_.max_depth) + \" vs. \" + str(depth))\n",
        "print(\"Changed hyperparameter is --> min_samples_split = \\t\" + str(dt.best_estimator_.min_samples_split) + \" vs. \" + str(split))\n",
        "\n",
        "#to see the best values found in our code before\n",
        "print(\"\\n*** DATA ***\")\n",
        "print(\"Best (found) min_samples_split = \" + str(Best_minSampl))\n",
        "print(\"Best (found) k-NN = \" + str(BestK))\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** BRUTE-FORCE ***\n",
            "Validation Accuracy = 70.16807%\n",
            "Testing Accuracy = 72.46377%\n",
            "\n",
            "*** Grid Search ***\n",
            "Testing Accuracy (gridDefault) = 73.51962%\n",
            "\n",
            "BONUS is Acquired by using GridSearch!\n",
            "\n",
            "*** ALL RESULTS ***\n",
            "Default Decision Tree Accuracy = \t68.59903%\n",
            "Fine-Tuned Decision Tree Accuracy = \t72.46377%\n",
            "KNN Accuracy = \t\t\t\t71.01449%\n",
            "GridSearch Accuracy = \t\t\t73.51962%\n",
            "Brute-Force Accuracy = \t\t\t72.46377%\n",
            "\n",
            "*** CHANGES with GridSearch ***\t\t\t\tGrid vs. Brute-Force\n",
            "Changed hyperparameter is --> max_depth = \t\t7 vs. 6\n",
            "Changed hyperparameter is --> min_samples_split = \t66 vs. 32\n",
            "\n",
            "*** DATA ***\n",
            "Best (found) min_samples_split = 22\n",
            "Best (found) k-NN = 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VCH10cwnAU-",
        "colab_type": "text"
      },
      "source": [
        "# Report: Write a summary of your approach to this problem; this should be like an abstract of a paper or the executive summary (you aim for clarity and passing on information, not going to details about known facts such as what decision trees are, assuming they are known to people in your research area).\n",
        "\n",
        "Must include statements such as:\n",
        "\n",
        "\n",
        "*   Include the problem definition: 1-2 lines\n",
        "*   Talk about train/val/test sets, size and how split.\n",
        "*   State what your test results are with the chosen method, parameters: e.g. \"We have obtained the best results with the ….. classifier (parameters=....) , giving classification accuracy of …% on test data….\"\n",
        "*   Comment on the speed of the algorithms and anything else that you deem important/interesting (e.g. confusion matrix)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72KDasqHnt1T",
        "colab_type": "text"
      },
      "source": [
        "# Write your report in this cell\n",
        "\n",
        "  The problem is to decide, based on the given data of some people, wheter the person is a good (reliable) or bad (unreliable). We are trying to fit an accurate decision tree that will help to decide later data of the people. There are 49 different inputs (features) such as credit amount, age, retirement info etc. and there is only one output (label) which are good or bad.\n",
        "\n",
        "  The number of data our training set (train_data) is 793 and our test size (test_data) is 207. First, we used all of the training data to create a decision tree with the criterion hyperparameter \"entropy\". Then, when we are finished with the basic decision tree, we test our data with the data set test_data and our accuracy usually stands between 65-70%. \n",
        "\n",
        "  Then, we want to improve our accuracy by changing the min_samples_split hyperparameter of the DecisionTreeClassifier. In order to increase our accuracy, we wanted to try different values between 2-100 for min_samples_split, and we created a for loop to iterate over the values of min_samples_split and an array to store the accuracies of the train data and validation data. The important thing here is splitting randomly our whole training data into 2 parts which are traing data (x_train) and validation data (x_val) with the ratio of 0.7 to 0.3. The training data is to train the decision tree and the rest is to validate the training results. After the for loop, we find the best validation accuracy with given min_samples_split. In that part, you can see a a little code which is to prevent overfitting. In the array of validation accuracies, I wanted to pick a values that maximizes the validation accuracy but not overfitting. So, I started the search from the index 20. When we find the best validation accuracy and the best resulted min_samples_split, we used it to fine-tune our decision tree. We created a decision tree with the criterion hyperparameter as \"entropy\" and min_samples_split as Best_minSampl. Then, we fit our test_data into our fine-tuned decision tree, and get an accuracy a bit better then the basic decision tree. The accuracy of the fine-tuned decision tree stands between 67-71%. \n",
        "  \n",
        "  Thirdly, we used k-Nearest Neigbor algorithm to find an accuracy. So, with different k values ranging between 1-30, we used the same data sets we splitted before. We used training data to train our model with the changing hyperparameter k. Then, we validate the model and store the accuracies in order to compater later on to decide which k value gives the best accuracy. When to loop ends, we find the best k value that gives best validation accuracy and we test our data with the found k value. The accuracy of k-NN algorithm usually stands between 70-72%. Slightly better than the fine-tuned decision tree.\n",
        "\n",
        "  Last but not least, as a bonus, we used grid-search algorithm to find what hyperparameters we could change to get a better result than the fine-tuned decision tree. For this part, I used a brute-force algorithm to change 2 hyperparameters, max_depth of the tree and min_samples_split, simultaneously. The brute-force algorithm is running 2 for loops and changing the hyperparameters and stores the training accuracies. Then, finding the best accuracy and the hyperparameters that satisfy the accuracy. In the end, we create a decision tree with the brute-force found max_depth and min_samples_split hyperparameters. The running time is O(nk) where n and k denotes the ranges of tried hyperparameter values. The accuracy of this usually stands between 66-69%, which is not efficient at all. However, the real algorithm, grid search, finds the best hyperparameters slower but the accuracy is much higher than the basic decision tree. In the grid search algorithm just changing the max_depth and min_samples_split hyperparatmeters increase our accuracy like 5% most of the time.\n",
        "\n",
        "  Finally, in order to obtain the best test accuracy, one should use the grid search algorithm with the changing hyperparameters max_depth and min_samples_split. The worst accuracy obtained by the basic decision tree with no given hyperparameters. (entropy is not included) K-NN and fine-tuned decision tree algorithms are not that complex or does not take long time but the accuracies are a bit lower than the grid search algorithm. The accuracy of the tests are inversely proportional to the complexity of the algorithm. \n",
        "  \n",
        "#To see the all accuracies and the hyperparameter values, please see the last coding block, which states nearly everything.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}